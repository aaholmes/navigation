{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bananas\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from dqn_agent import Agent\n",
    "from train import dqn\n",
    "from utils import scores_to_file\n",
    "\n",
    "\n",
    "env = UnityEnvironment(file_name=\"Banana.app\", no_graphics=True)\n",
    "\n",
    "# Select the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN\n",
    "agent = Agent(state_size, action_size, seed=0, update_every=4)\n",
    "scores = dqn(env, agent, brain_name, eps_start=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double DQN, choose moves using mean(Q1, Q2)\n",
    "agent = Agent(state_size, action_size, seed=0, ddqn=True)\n",
    "scores = dqn(env, agent, brain_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double DQN, choose moves using random_choice(Q1, Q2)\n",
    "agent = Agent(state_size, action_size, seed=0, ddqn=True, ddqn_mean=False)\n",
    "scores = dqn(env, agent, brain_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores from one run\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Double DQN agent that moves using mean(Q1, Q2) with gamma = 0.95\n",
      "Initializing replay buffer with buffer size 30000 and batch size 64\n",
      "Episode 8\tEps: 0.384\tScore: 1\tMax Score: 3\tAverage Score Last 10: 0.75\tAverage Score Last 100: 0.75.50\t\n",
      "Episode 16\tEps: 0.369\tScore: 1\tMax Score: 8\tAverage Score Last 10: 3.00\tAverage Score Last 100: 2.00\t\n",
      "Episode 24\tEps: 0.355\tScore: 0\tMax Score: 9\tAverage Score Last 10: 3.80\tAverage Score Last 100: 2.582\t\n",
      "Episode 33\tEps: 0.339\tScore: 2\tMax Score: 13\tAverage Score Last 10: 3.70\tAverage Score Last 100: 3.006\t\n",
      "Episode 41\tEps: 0.326\tScore: 10\tMax Score: 13\tAverage Score Last 10: 6.90\tAverage Score Last 100: 3.95\t\n",
      "Episode 49\tEps: 0.313\tScore: 6\tMax Score: 13\tAverage Score Last 10: 6.60\tAverage Score Last 100: 4.35\t\n",
      "Episode 58\tEps: 0.299\tScore: 10\tMax Score: 13\tAverage Score Last 10: 6.90\tAverage Score Last 100: 4.76\t\n",
      "Episode 66\tEps: 0.287\tScore: 3\tMax Score: 13\tAverage Score Last 10: 6.10\tAverage Score Last 100: 4.86\t\n",
      "Episode 74\tEps: 0.276\tScore: 10\tMax Score: 13\tAverage Score Last 10: 6.20\tAverage Score Last 100: 5.01\t\n",
      "Episode 83\tEps: 0.264\tScore: 9\tMax Score: 15\tAverage Score Last 10: 8.70\tAverage Score Last 100: 5.405\t\n",
      "Episode 91\tEps: 0.253\tScore: 12\tMax Score: 15\tAverage Score Last 10: 9.50\tAverage Score Last 100: 5.76\t\n",
      "Episode 99\tEps: 0.244\tScore: 11\tMax Score: 15\tAverage Score Last 10: 9.60\tAverage Score Last 100: 6.071\t\n",
      "Episode 108\tEps: 0.233\tScore: 14\tMax Score: 15\tAverage Score Last 10: 8.90\tAverage Score Last 100: 6.73\t\n",
      "Episode 116\tEps: 0.224\tScore: 9\tMax Score: 16\tAverage Score Last 10: 9.50\tAverage Score Last 100: 7.246\t\n",
      "Episode 124\tEps: 0.215\tScore: 10\tMax Score: 16\tAverage Score Last 10: 9.80\tAverage Score Last 100: 7.733\t\n",
      "Episode 133\tEps: 0.205\tScore: 12\tMax Score: 16\tAverage Score Last 10: 10.50\tAverage Score Last 100: 8.31\t\n",
      "Episode 141\tEps: 0.197\tScore: 10\tMax Score: 19\tAverage Score Last 10: 11.40\tAverage Score Last 100: 8.62\t\n",
      "Episode 149\tEps: 0.190\tScore: 10\tMax Score: 19\tAverage Score Last 10: 10.60\tAverage Score Last 100: 8.97\t\n",
      "Episode 158\tEps: 0.181\tScore: 17\tMax Score: 19\tAverage Score Last 10: 11.80\tAverage Score Last 100: 9.42\t\n",
      "Episode 166\tEps: 0.174\tScore: 6\tMax Score: 19\tAverage Score Last 10: 11.90\tAverage Score Last 100: 9.843\t\n",
      "Episode 174\tEps: 0.167\tScore: 8\tMax Score: 19\tAverage Score Last 10: 10.50\tAverage Score Last 100: 10.268\t\n",
      "Episode 183\tEps: 0.160\tScore: 13\tMax Score: 19\tAverage Score Last 10: 12.20\tAverage Score Last 100: 10.63\t\n",
      "Episode 191\tEps: 0.154\tScore: 12\tMax Score: 19\tAverage Score Last 10: 12.70\tAverage Score Last 100: 10.86\t\n",
      "Episode 199\tEps: 0.148\tScore: 13\tMax Score: 19\tAverage Score Last 10: 11.80\tAverage Score Last 100: 11.03\t\n",
      "Episode 208\tEps: 0.141\tScore: 12\tMax Score: 19\tAverage Score Last 10: 12.40\tAverage Score Last 100: 11.36\t\n",
      "Episode 216\tEps: 0.135\tScore: 12\tMax Score: 19\tAverage Score Last 10: 13.70\tAverage Score Last 100: 11.70\t\n",
      "Episode 224\tEps: 0.130\tScore: 10\tMax Score: 19\tAverage Score Last 10: 12.90\tAverage Score Last 100: 11.89\t\n",
      "Episode 233\tEps: 0.124\tScore: 8\tMax Score: 19\tAverage Score Last 10: 11.90\tAverage Score Last 100: 12.037\t\n",
      "Episode 241\tEps: 0.120\tScore: 8\tMax Score: 19\tAverage Score Last 10: 12.90\tAverage Score Last 100: 12.180\t\n",
      "Episode 249\tEps: 0.115\tScore: 9\tMax Score: 19\tAverage Score Last 10: 10.50\tAverage Score Last 100: 12.145\t\n",
      "Episode 258\tEps: 0.110\tScore: 14\tMax Score: 20\tAverage Score Last 10: 13.60\tAverage Score Last 100: 12.33\t\n",
      "Episode 266\tEps: 0.105\tScore: 14\tMax Score: 20\tAverage Score Last 10: 13.00\tAverage Score Last 100: 12.50\t\n",
      "Episode 274\tEps: 0.101\tScore: 13\tMax Score: 21\tAverage Score Last 10: 14.00\tAverage Score Last 100: 12.70\t\n",
      "Episode 283\tEps: 0.097\tScore: 14\tMax Score: 21\tAverage Score Last 10: 12.10\tAverage Score Last 100: 12.64\t\n",
      "Episode 291\tEps: 0.093\tScore: 18\tMax Score: 21\tAverage Score Last 10: 14.30\tAverage Score Last 100: 12.82\t\n",
      "Episode 298\tEps: 0.090\tScore: 20\tMax Score: 21\tAverage Score Last 10: 14.60\tAverage Score Last 100: 13.01\n",
      "Environment solved in 198 episodes!\tEps: 0.090\tAverage Score: 13.01\n"
     ]
    }
   ],
   "source": [
    "# Run all 3 models 3 times each\n",
    "for seed in range(2, 3):\n",
    "    # DQN\n",
    "    #agent = Agent(state_size, action_size, seed, update_every=4)\n",
    "    #scores = dqn(env, agent, brain_name, eps_start=0.4)\n",
    "    #scores_to_file(scores, \"results/dqn/\" + str(seed) + \".txt\")\n",
    "    # Double DQN, choose moves using mean(Q1, Q2)\n",
    "    agent = Agent(state_size, action_size, seed, ddqn=True)\n",
    "    scores = dqn(env, agent, brain_name, eps_start=0.4, eps_decay=0.99)\n",
    "    scores_to_file(scores, \"results/ddqn/\" + str(seed) + \".txt\")\n",
    "    # Double DQN, choose moves using random_choice(Q1, Q2)\n",
    "    #agent = Agent(state_size, action_size, seed, ddqn=True, ddqn_mean=False)\n",
    "    #scores = dqn(env, agent, brain_name)\n",
    "    #scores_to_file(scores, \"results/ddqn_rand/\" + str(seed) + \".txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN\n",
    "agent = Agent(state_size, action_size, seed=0, update_every=4)\n",
    "scores = dqn(env, agent, brain_name, eps_start=0.4)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec4afbf0c90b25735c4ffe7e95247dd3a62fcc462b9d302acd8b15dfa91b9bf1"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('drlnd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
